<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.75.1 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="Aaron">
<meta name="keywords" content="nlp, text classification, keras, machine learning, deep learning, embedding, tf-idf, python">
<meta name="description" content="本文语料仍然是上篇所用的搜狗新闻语料，采用中文预训练词向量模型对词进行向量表示。上篇文章将文本分词之后，采用了TF-IDF的特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，本篇采用的词向量是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。本文代码在GitHub上。">


<meta property="og:description" content="本文语料仍然是上篇所用的搜狗新闻语料，采用中文预训练词向量模型对词进行向量表示。上篇文章将文本分词之后，采用了TF-IDF的特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，本篇采用的词向量是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。本文代码在GitHub上。">
<meta property="og:type" content="article">
<meta property="og:title" content="基于预训练词向量模型的文本分类方法">
<meta name="twitter:title" content="基于预训练词向量模型的文本分类方法">
<meta property="og:url" content="https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
<meta property="twitter:url" content="https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
<meta property="og:site_name" content="Aaron&#39;s blog">
<meta property="og:description" content="本文语料仍然是上篇所用的搜狗新闻语料，采用中文预训练词向量模型对词进行向量表示。上篇文章将文本分词之后，采用了TF-IDF的特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，本篇采用的词向量是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。本文代码在GitHub上。">
<meta name="twitter:description" content="本文语料仍然是上篇所用的搜狗新闻语料，采用中文预训练词向量模型对词进行向量表示。上篇文章将文本分词之后，采用了TF-IDF的特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，本篇采用的词向量是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。本文代码在GitHub上。">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2019-05-29T21:15:48">
  
  
    <meta property="article:modified_time" content="2019-05-29T21:15:48">
  
  
  
    
      <meta property="article:section" content="data science">
    
  
  
    
      <meta property="article:tag" content="nlp">
    
      <meta property="article:tag" content="text classification">
    
      <meta property="article:tag" content="keras">
    
      <meta property="article:tag" content="machine learning">
    
      <meta property="article:tag" content="deep learning">
    
      <meta property="article:tag" content="embedding">
    
      <meta property="article:tag" content="tf-idf">
    
      <meta property="article:tag" content="python">
    
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@lijqhs">


  <meta name="twitter:creator" content="@lijqhs">






  <meta property="og:image" content="//cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/gallery/u/oskars-sylwan-QhCp-bu1aw8-unsplash.jpg">
  <meta property="twitter:image" content="//cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/gallery/u/oskars-sylwan-QhCp-bu1aw8-unsplash.jpg">


  <meta property="og:image" content="//cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/gallery/u/oskars-sylwan-QhCp-bu1aw8-unsplash.jpg">
  <meta property="twitter:image" content="//cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/gallery/u/oskars-sylwan-QhCp-bu1aw8-unsplash.jpg">




  <meta property="og:image" content="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png">
  <meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png">


    <title>基于预训练词向量模型的文本分类方法</title>

    <link rel="icon" href="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png">
    

    

    <link rel="canonical" href="https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://lijqhs.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-182241114-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://lijqhs.github.io/">Aaron&#39;s blog</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://lijqhs.github.io/#about">
    
    
    
      
        <img class="header-picture" src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://lijqhs.github.io/#about">
          <img class="sidebar-profile-picture" src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Aaron</h4>
        
          <h5 class="sidebar-profile-bio">a humble learner</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/lijqhs" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://twitter.com/lijqhs" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-twitter"></i>
      
      <span class="sidebar-button-desc">Twitter</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/jiaqingli" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="mailto:lijqhs@gmail.com" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-envelope"></i>
      
      <span class="sidebar-button-desc">Email Me</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://lijqhs.github.io/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      
  <div class="post-header-cover
              text-left
              "
       style="background-image:url('//cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/gallery/u/oskars-sylwan-QhCp-bu1aw8-unsplash.jpg')"
       data-behavior="4">
    
      <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      基于预训练词向量模型的文本分类方法
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2019-05-29T21:15:48&#43;08:00">
        
  May 29, 2019

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://lijqhs.github.io/categories/data-science">data science</a>
    
  

  </div>

</div>
    
  </div>


      <div id="main" data-behavior="4"
        class="hasCover
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>本文语料仍然是上篇所用的搜狗新闻语料，采用中文预训练词向量模型对词进行向量表示。<a href="https://lijqhs.github.io/2019/05/text-classification-scikit-learn/">上篇文章</a>将文本分词之后，采用了TF-IDF的特征提取方式对文本进行向量化表示，所产生的文本表示矩阵是一个稀疏矩阵，本篇采用的词向量是一个稠密向量，可以理解为将文本的语义抽象信息嵌入到了一个具体的多维空间中，词之间语义关系可以用向量空间中的范数计算来表示。本文代码在<a href="https://github.com/lijqhs/text-classification-cn">GitHub</a>上。</p>
<p>目录:</p>
<ul>
<li><a href="#1-%E8%AF%BB%E5%8F%96%E8%AF%AD%E6%96%99">1. 读取语料</a></li>
<li><a href="#2-%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B">2. 加载预训练词向量模型</a></li>
<li><a href="#3-%E4%BD%BF%E7%94%A8keras%E5%AF%B9%E8%AF%AD%E6%96%99%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86">3. 使用Keras对语料进行处理</a></li>
<li><a href="#4-%E5%AE%9A%E4%B9%89%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9F%A9%E9%98%B5">4. 定义词嵌入矩阵</a>
<ul>
<li><a href="#embedding-layer">Embedding Layer</a></li>
</ul>
</li>
<li><a href="#5-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B">5. 构建模型</a></li>
<li><a href="#6-%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">6. 参考资料</a></li>
</ul>
<h2 id="1-读取语料">1. 读取语料</h2>
<p>读取语料与上一篇略有区别，这里读取原始语料，划分训练集和测试集，放在了后面预处理部分。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">texts, labels <span style="color:#f92672">=</span> load_raw_datasets()
</code></pre></div><pre><code>label: C000008, len: 1990
label: C000010, len: 1990
label: C000013, len: 1990
label: C000014, len: 1990
label: C000016, len: 1990
label: C000020, len: 1990
label: C000022, len: 1990
label: C000023, len: 1990
label: C000024, len: 1990

Done. 9 total categories, 17910 total docs. cost 224.0759744644165 seconds.
</code></pre>
<p>注意到，所有文本读入到<code>texts</code>列表中，标签信息读入到<code>labels</code>列表中。每个文本所在目录名称作为分类标签，在创建<code>labels</code>列表时，代码中做了一步处理，读入目录时的序号存入<code>labels</code>列表中（第一个标签为0，第二个标签为1，依次类推），后面在使用keras对文本进行预处理的过程中，将调用<code>to_categorical</code>将整数类别标签转为向量分类编码，这是为了使用分类交叉熵损失函数<code>categorical_crossentropy</code>。这里可以了解一下关于各类统计变量间的区别：<a href="https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/">Difference between categorical and ordinal variables</a>。</p>
<p>下表是转换后的标签表示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">标签</th>
<th style="text-align:center">名称</th>
<th style="text-align:center">分类编码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">C000008</td>
<td style="text-align:center">Finance</td>
<td style="text-align:center">[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">C000010</td>
<td style="text-align:center">IT</td>
<td style="text-align:center">[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">C000013</td>
<td style="text-align:center">Health</td>
<td style="text-align:center">[0, 0, 1, 0, 0, 0, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">C000014</td>
<td style="text-align:center">Sports</td>
<td style="text-align:center">[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">C000016</td>
<td style="text-align:center">Travel</td>
<td style="text-align:center">[0, 0, 0, 0, 1, 0, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">C000020</td>
<td style="text-align:center">Education</td>
<td style="text-align:center">[0, 0, 0, 0, 0, 1, 0, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">C000022</td>
<td style="text-align:center">Recruit</td>
<td style="text-align:center">[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">C000023</td>
<td style="text-align:center">Culture</td>
<td style="text-align:center">[0, 0, 0, 0, 0, 0, 0, 1, 0]</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">C000024</td>
<td style="text-align:center">Military</td>
<td style="text-align:center">[0, 0, 0, 0, 0, 0, 0, 0, 1]</td>
</tr>
</tbody>
</table>
<p>分类编码其实就是one-hot编码，可以用代码解释一下这个转换过程：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">to_one_hot</span>(labels):
    one_hot <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(labels), len(labels)))
    <span style="color:#66d9ef">for</span> i, label <span style="color:#f92672">in</span> enumerate(labels):
        one_hot[i, label] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span>
    <span style="color:#66d9ef">return</span> one_hot

<span style="color:#66d9ef">print</span>(to_one_hot([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">7</span>,<span style="color:#ae81ff">8</span>]))
</code></pre></div><p>在keras有内置函数</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> keras.utils <span style="color:#f92672">import</span> to_categorical
<span style="color:#66d9ef">print</span>(to_categorical([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">7</span>,<span style="color:#ae81ff">8</span>]))
</code></pre></div><p>如果想要将向量分类编码转回整数类别标签，可以用<code>numpy</code>中的<code>argmax</code>函数：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> argmax
argmax(labels[<span style="color:#ae81ff">0</span>])
</code></pre></div><h2 id="2-加载预训练词向量模型">2. 加载预训练词向量模型</h2>
<p>解压之后的中文预训练词向量模型的文件格式是文本文件，首行只有两个空格隔开的数字：词的个数和词向量的维度，从第二行开始格式为：词 数字1 数字2 …… 数字300，形式如下：</p>
<blockquote>
<p>364180 300
人民文学出版社 0.003146 0.582671 0.049029 -0.312803 0.522986 0.026432 -0.097115 0.194231 -0.362708 &hellip;&hellip;
&hellip;&hellip;</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 读取预训练模型</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
embeddings_index <span style="color:#f92672">=</span> {}
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;Embedding/sgns.sogou.word&#39;</span>) <span style="color:#66d9ef">as</span> f:
    num, embedding_dim <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>split()

    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> f:
        values <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split()
        word <span style="color:#f92672">=</span> values[<span style="color:#ae81ff">0</span>]
        coefs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(values[<span style="color:#ae81ff">1</span>:], dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;float32&#39;</span>)
        embeddings_index[word] <span style="color:#f92672">=</span> coefs

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Found </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> word vectors, dimension </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (len(embeddings_index), embedding_dim))
</code></pre></div><pre><code>Found 364180 word vectors, dimension 300
</code></pre>
<h2 id="3-使用keras对语料进行处理">3. 使用Keras对语料进行处理</h2>
<p>在<a href="https://lijqhs.github.io/2019/05/text-classification-scikit-learn/">上篇文章</a>中，我们使用了TfidfVectorizer，将训练语料转换为TFIDF矩阵，每个向量的长度相同（等于总语料库词汇量的大小）。本文将使用Keras中的Tokenizer对文本进行处理，每个向量等于每个文本的长度，这个长度在处理的时候由变量<code>MAX_SEQUENCE_LEN</code>做了限制，其数值并不表示计数，而是对应于字典<code>tokenizer.word_index</code>中的单词索引值，这个字典是在调用<code>Tokenizer</code>时产生。</p>
<p>长度超过<code>MAX_SEQUENCE_LEN</code>的文本序列会被截断，长度小于这个值的文本序列则需要补零来达到这个长度，<code>Keras</code>中的<code>pad_sequence()</code>就是用零来填充向量序列。形式如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pad_sequences([[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">5</span>],[<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">7</span>,<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">9</span>]], maxlen<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
array([[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>],
       [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">9</span>]], dtype<span style="color:#f92672">=</span>int32)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> keras.preprocessing.text <span style="color:#f92672">import</span> Tokenizer
<span style="color:#f92672">from</span> keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences
<span style="color:#f92672">from</span> keras.utils <span style="color:#f92672">import</span> to_categorical
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

MAX_SEQUENCE_LEN <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>  <span style="color:#75715e"># 文档限制长度</span>
MAX_WORDS_NUM <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>  <span style="color:#75715e"># 词典的个数</span>
VAL_SPLIT_RATIO <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span> <span style="color:#75715e"># 验证集的比例</span>

tokenizer <span style="color:#f92672">=</span> Tokenizer(num_words<span style="color:#f92672">=</span>MAX_WORDS_NUM)
tokenizer<span style="color:#f92672">.</span>fit_on_texts(texts)
sequences <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences(texts)

word_index <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>word_index
<span style="color:#66d9ef">print</span>(len(word_index)) <span style="color:#75715e"># all token found</span>
<span style="color:#75715e"># print(word_index.get(&#39;新闻&#39;)) # get word index</span>
dict_swaped <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> _dict: {val:key <span style="color:#66d9ef">for</span> (key, val) <span style="color:#f92672">in</span> _dict<span style="color:#f92672">.</span>items()}
word_dict <span style="color:#f92672">=</span> dict_swaped(word_index) <span style="color:#75715e"># swap key-value</span>
data <span style="color:#f92672">=</span> pad_sequences(sequences, maxlen<span style="color:#f92672">=</span>MAX_SEQUENCE_LEN)

labels_categorical <span style="color:#f92672">=</span> to_categorical(np<span style="color:#f92672">.</span>asarray(labels))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Shape of data tensor:&#39;</span>, data<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Shape of label tensor:&#39;</span>, labels_categorical<span style="color:#f92672">.</span>shape)

indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(data<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(indices)
data <span style="color:#f92672">=</span> data[indices]
labels_categorical <span style="color:#f92672">=</span> labels_categorical[indices]

<span style="color:#75715e"># split data by ratio</span>
val_samples_num <span style="color:#f92672">=</span> int(VAL_SPLIT_RATIO <span style="color:#f92672">*</span> data<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])

x_train <span style="color:#f92672">=</span> data[:<span style="color:#f92672">-</span>val_samples_num]
y_train <span style="color:#f92672">=</span> labels_categorical[:<span style="color:#f92672">-</span>val_samples_num]
x_val <span style="color:#f92672">=</span> data[<span style="color:#f92672">-</span>val_samples_num:]
y_val <span style="color:#f92672">=</span> labels_categorical[<span style="color:#f92672">-</span>val_samples_num:]
</code></pre></div><pre><code>263284
Shape of data tensor: (17910, 1000)
Shape of label tensor: (17910, 9)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">list(word_index<span style="color:#f92672">.</span>items())[<span style="color:#ae81ff">90</span>]
</code></pre></div><pre><code>('儿童', 958)
</code></pre>
<p>代码中<code>word_index</code>表示发现的所有词，得到的文本序列取的是<code>word_index</code>中前面20000个词对应的索引，文本序列集合中的所有词的索引号都在20000之前：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">len(data[data<span style="color:#f92672">&gt;=</span><span style="color:#ae81ff">20000</span>])
<span style="color:#ae81ff">0</span>
</code></pre></div><p>我们可以通过生成的词索引序列和对应的索引词典查看原始文本和对应的标签：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># convert from index to origianl doc</span>
<span style="color:#66d9ef">for</span> w_index <span style="color:#f92672">in</span> data[<span style="color:#ae81ff">0</span>]:
    <span style="color:#66d9ef">if</span> w_index <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(word_dict[w_index], end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39; &#39;</span>)
</code></pre></div><pre><code>昆虫 大自然 歌手 昆虫 口腔 发出 昆虫 界 著名 腹部 一对 是从 发出 外面 一对 弹性 称作 声 肌 相连 发音 肌 收缩 振动 声音 空间 响亮 传到 ５ ０ ０ 米 求婚 听到 发音 部位 发音 声音 两 张开 蚊子 一对 边缘 支撑 两只 每秒 ２ ５ ０ ～ ６ ０ ０ 次 推动 空气 往返 运动 发出 微弱 声 来源 语文
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">category_labels[dict_swaped(labels_index)[argmax(labels_categorical[<span style="color:#ae81ff">0</span>])]]
</code></pre></div><pre><code>'_20_Education'
</code></pre>
<h2 id="4-定义词嵌入矩阵">4. 定义词嵌入矩阵</h2>
<p>下面创建一个词嵌入矩阵，用来作为上述文本集合词典（只取序号在前<code>MAX_WORDS_NUM</code>的词，对应了比较常见的词）的词嵌入矩阵，矩阵维度是<code>(MAX_WORDS_NUM, EMBEDDING_DIM)</code>。</p>
<p>矩阵的每一行<code>i</code>代表词典<code>word_index</code>中第<code>i</code>个词的词向量。这个词嵌入矩阵是预训练词向量的一个子集。语料中很可能有的词不在预训练词向量中，这样的词在这个词向量矩阵中对应的向量元素都设为零。在本例中，20000个词有92.35%在预训练词向量中。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">EMBEDDING_DIM <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span> <span style="color:#75715e"># embedding dimension</span>

embedding_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((MAX_WORDS_NUM<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, EMBEDDING_DIM)) <span style="color:#75715e"># row 0 for 0</span>
<span style="color:#66d9ef">for</span> word, i <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items():
    embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word)
    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> MAX_WORDS_NUM:
        <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            <span style="color:#75715e"># Words not found in embedding index will be all-zeros.</span>
            embedding_matrix[i] <span style="color:#f92672">=</span> embedding_vector
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">nonzero_elements <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>count_nonzero(np<span style="color:#f92672">.</span>count_nonzero(embedding_matrix, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
nonzero_elements <span style="color:#f92672">/</span> MAX_WORDS_NUM
</code></pre></div><pre><code>0.9235
</code></pre>
<h3 id="embedding-layer">Embedding Layer</h3>
<p>下面代码中用<code>keras</code>构建神经网络文本分类模型时，用到了嵌入层<code>Embedding</code>。关于嵌入层（<a href="https://keras.io/layers/embeddings/">Keras Embedding Layer API</a>），<a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/">这篇文章</a>有比较详细的介绍。嵌入层的目的是将输入序列中的整数索引转换成一个稠密的向量，嵌入层的输入是一个2D张量，形状为<code>(batch_size, sequence_length)</code>，输出是3D张量，形状为<code>(batch_size, sequence_length, output_dim)</code>。</p>
<p>嵌入层的输入数据<code>sequence</code>向量的整数元素对应词的编码，前面看到这个获取序列编码的步骤使用了Keras的<code>Tokenizer API</code>来实现，如果不使用预训练词向量模型，嵌入层是用随机权重进行初始化，在训练中将学习到训练集中的所有词的权重，也就是词向量。在定义<code>Embedding</code>层，需要至少3个输入数据：</p>
<ul>
<li><code>input_dim</code>：文本词典的大小，本例中就是<code>MAX_WORDS_NUM + 1</code></li>
<li><code>output_dim</code>：词嵌入空间的维度，就是词向量的长度，本例中对应<code>EMBEDDING_DIM</code></li>
<li><code>input_length</code>：这是输入序列的长度，本例中对应<code>MAX_SEQUENCE_LEN</code></li>
</ul>
<p>本文中用到了另外两个输入参数<code>weights=[embedding_matrix]</code>和<code>trainable=False</code>，前者设置该层的嵌入矩阵为上面我们定义好的词嵌入矩阵，即不使用随机初始化的权重，后者设置为本层参数不可训练，即不会随着后面模型的训练而更改。这里涉及了<code>Embedding</code>层的几种使用方式：</p>
<ul>
<li>从头开始训练出一个词向量，保存之后可以用在其他的训练任务中</li>
<li>嵌入层作为深度学习的第一个隐藏层，本身就是深度学习模型训练的一部分</li>
<li>加载预训练词向量模型，这是一种迁移学习，本文就是这样的示例</li>
</ul>
<h2 id="5-构建模型">5. 构建模型</h2>
<p>Keras支持两种类型的模型结构：</p>
<ul>
<li>Sequential类，顺序模型，这个仅用于层的线性堆叠，最常见的网络架构</li>
<li>Functional API，函数式API，用于层组成的有向无环图，可以构建任意形式的架构</li>
</ul>
<p>为了有个对比，我们先不加载预训练模型，让模型自己训练词权重向量。<code>Flatten</code>层用来将输入“压平”，即把多维的输入一维化，这是嵌入层的输出转入全连接层(<code>Dense</code>)的必需的过渡。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">input_dim <span style="color:#f92672">=</span> x_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
model1 <span style="color:#f92672">=</span> Sequential()
model1<span style="color:#f92672">.</span>add(Embedding(input_dim<span style="color:#f92672">=</span>MAX_WORDS_NUM<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,
                    output_dim<span style="color:#f92672">=</span>EMBEDDING_DIM,
                    input_length<span style="color:#f92672">=</span>MAX_SEQUENCE_LEN))
model1<span style="color:#f92672">.</span>add(Flatten())
model1<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(input_dim,)))
model1<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))
model1<span style="color:#f92672">.</span>add(Dense(len(labels_index), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>))

model1<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rmsprop&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

history1 <span style="color:#f92672">=</span> model1<span style="color:#f92672">.</span>fit(x_train,
                    y_train,
                    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,
                    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
                    validation_data<span style="color:#f92672">=</span>(x_val, y_val))
</code></pre></div><pre><code>Train on 14328 samples, validate on 3582 samples
Epoch 1/30
14328/14328 [==============================] - 59s 4ms/step - loss: 3.1273 - acc: 0.2057 - val_loss: 1.9355 - val_acc: 0.2510
Epoch 2/30
14328/14328 [==============================] - 56s 4ms/step - loss: 2.0853 - acc: 0.3349 - val_loss: 1.8037 - val_acc: 0.3473
Epoch 3/30
14328/14328 [==============================] - 56s 4ms/step - loss: 1.7210 - acc: 0.4135 - val_loss: 1.2498 - val_acc: 0.5731
......
Epoch 29/30
14328/14328 [==============================] - 56s 4ms/step - loss: 0.5843 - acc: 0.8566 - val_loss: 1.3564 - val_acc: 0.6516
Epoch 30/30
14328/14328 [==============================] - 56s 4ms/step - loss: 0.5864 - acc: 0.8575 - val_loss: 0.5970 - val_acc: 0.8501
</code></pre>
<p>每个Keras层都提供了获取或设置本层权重参数的方法：</p>
<ul>
<li><code>layer.get_weights()</code>：返回层的权重（<code>numpy array</code>）</li>
<li><code>layer.set_weights(weights)</code>：从<code>numpy array</code>中将权重加载到该层中，要求<code>numpy array</code>的形状与<code>layer.get_weights()</code>的形状相同</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">embedding_custom <span style="color:#f92672">=</span> model1<span style="color:#f92672">.</span>layers[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get_weights()[<span style="color:#ae81ff">0</span>]
embedding_custom
</code></pre></div><pre><code>array([[ 0.39893672, -0.9062594 ,  0.35500282, ..., -0.73564297,
         0.50492775, -0.39815223],
       [ 0.10640696,  0.18888871,  0.05909824, ..., -0.1642032 ,
        -0.02778293, -0.15340094],
       [ 0.06566656, -0.04023357,  0.1276007 , ...,  0.04459211,
         0.08887506,  0.05389333],
       ...,
       [-0.12710813, -0.08472785, -0.2296919 , ...,  0.0468552 ,
         0.12868881,  0.18596107],
       [-0.03790742,  0.09758633,  0.02123675, ..., -0.08180046,
         0.10254312,  0.01284804],
       [-0.0100647 ,  0.01180602,  0.00446023, ...,  0.04730382,
        -0.03696882,  0.00119566]], dtype=float32)
</code></pre>
<p><code>get_weights</code>方法得到的就是词嵌入矩阵，如果本例中取的词典足够大，这样的词嵌入矩阵就可以保存下来，作为其他任务的预训练模型使用。通过<code>get_config()</code>可以获取每一层的配置信息：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model1<span style="color:#f92672">.</span>layers[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get_config()
</code></pre></div><pre><code>{'activity_regularizer': None,
 'batch_input_shape': (None, 1000),
 'dtype': 'float32',
 'embeddings_constraint': None,
 'embeddings_initializer': {'class_name': 'RandomUniform',
  'config': {'maxval': 0.05, 'minval': -0.05, 'seed': None}},
 'embeddings_regularizer': None,
 'input_dim': 20001,
 'input_length': 1000,
 'mask_zero': False,
 'name': 'embedding_13',
 'output_dim': 300,
 'trainable': True}
</code></pre>
<p>可以将模型训练的结果打印出来</p>
<p><img src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.0/img/post/acc-loss-model1.png" alt="acc-loss-model1.png"></p>
<p>第一个模型训练时间花了大约30分钟训练完30个epoch，下面第二个模型在第一个模型基础上加载词嵌入矩阵，看是否可以提高训练的效率。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model2 <span style="color:#f92672">=</span> Sequential()
model2<span style="color:#f92672">.</span>add(Embedding(input_dim<span style="color:#f92672">=</span>MAX_WORDS_NUM<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,
                    output_dim<span style="color:#f92672">=</span>EMBEDDING_DIM,
                    weights<span style="color:#f92672">=</span>[embedding_matrix],
                    input_length<span style="color:#f92672">=</span>MAX_SEQUENCE_LEN,
                    trainable<span style="color:#f92672">=</span>False))
model2<span style="color:#f92672">.</span>add(Flatten())
model2<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(input_dim,)))
model2<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))
model2<span style="color:#f92672">.</span>add(Dense(len(labels_index), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>))

model2<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rmsprop&#39;</span>,
              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

history2 <span style="color:#f92672">=</span> model2<span style="color:#f92672">.</span>fit(x_train,
                    y_train,
                    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
                    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
                    validation_data<span style="color:#f92672">=</span>(x_val, y_val))
</code></pre></div><pre><code>Train on 14328 samples, validate on 3582 samples
Epoch 1/10
14328/14328 [==============================] - 37s 3ms/step - loss: 1.3124 - acc: 0.6989 - val_loss: 0.7446 - val_acc: 0.8088
Epoch 2/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.2831 - acc: 0.9243 - val_loss: 0.5712 - val_acc: 0.8551
Epoch 3/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.1183 - acc: 0.9704 - val_loss: 0.6261 - val_acc: 0.8624
Epoch 4/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0664 - acc: 0.9801 - val_loss: 0.6897 - val_acc: 0.8607
Epoch 5/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0549 - acc: 0.9824 - val_loss: 0.7199 - val_acc: 0.8660
Epoch 6/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0508 - acc: 0.9849 - val_loss: 0.7261 - val_acc: 0.8582
Epoch 7/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0513 - acc: 0.9865 - val_loss: 0.8251 - val_acc: 0.8585
Epoch 8/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0452 - acc: 0.9858 - val_loss: 0.7891 - val_acc: 0.8707
Epoch 9/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0469 - acc: 0.9865 - val_loss: 0.8663 - val_acc: 0.8680
Epoch 10/10
14328/14328 [==============================] - 35s 2ms/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.9048 - val_acc: 0.8640
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.0/img/post/acc-loss-model2.png" alt="acc-loss-model2.png"></p>
<p>从第二个模型训练结果可以看到预训练模型的加载可以大幅提高模型训练的效率，模型的验证准确度也提升的比较快，但是同时发现在训练集上出现了过拟合的情况。</p>
<p>第三个模型的结构来自于Keras作者的博客<a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">示例</a>，这是CNN用于文本分类的例子。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">embedding_layer <span style="color:#f92672">=</span> Embedding(input_dim<span style="color:#f92672">=</span>MAX_WORDS_NUM<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>,
                            output_dim<span style="color:#f92672">=</span>EMBEDDING_DIM,
                            weights<span style="color:#f92672">=</span>[embedding_matrix],
                            input_length<span style="color:#f92672">=</span>MAX_SEQUENCE_LEN,
                            trainable<span style="color:#f92672">=</span>False)

sequence_input <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>(MAX_SEQUENCE_LEN,), dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>)
embedded_sequences <span style="color:#f92672">=</span> embedding_layer(sequence_input)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(embedded_sequences)
x <span style="color:#f92672">=</span> MaxPooling1D(<span style="color:#ae81ff">5</span>)(x)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
x <span style="color:#f92672">=</span> MaxPooling1D(<span style="color:#ae81ff">5</span>)(x)
x <span style="color:#f92672">=</span> Conv1D(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">5</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
x <span style="color:#f92672">=</span> MaxPooling1D(<span style="color:#ae81ff">35</span>)(x)  <span style="color:#75715e"># global max pooling</span>
x <span style="color:#f92672">=</span> Flatten()(x)
x <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(x)
preds <span style="color:#f92672">=</span> Dense(len(labels_index), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)(x)

model3 <span style="color:#f92672">=</span> Model(sequence_input, preds)
model3<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
              optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;rmsprop&#39;</span>,
              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;acc&#39;</span>])

history3 <span style="color:#f92672">=</span> model3<span style="color:#f92672">.</span>fit(x_train,
                    y_train,
                    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>,
                    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
                    validation_data<span style="color:#f92672">=</span>(x_val, y_val))
</code></pre></div><pre><code>Train on 14328 samples, validate on 3582 samples
Epoch 1/6
14328/14328 [==============================] - 77s 5ms/step - loss: 0.9943 - acc: 0.6719 - val_loss: 0.5129 - val_acc: 0.8582
Epoch 2/6
14328/14328 [==============================] - 76s 5ms/step - loss: 0.4841 - acc: 0.8571 - val_loss: 0.3929 - val_acc: 0.8841
Epoch 3/6
14328/14328 [==============================] - 77s 5ms/step - loss: 0.3483 - acc: 0.8917 - val_loss: 0.4022 - val_acc: 0.8724
Epoch 4/6
14328/14328 [==============================] - 77s 5ms/step - loss: 0.2763 - acc: 0.9100 - val_loss: 0.3441 - val_acc: 0.8942
Epoch 5/6
14328/14328 [==============================] - 76s 5ms/step - loss: 0.2194 - acc: 0.9259 - val_loss: 0.3014 - val_acc: 0.9107
Epoch 6/6
14328/14328 [==============================] - 77s 5ms/step - loss: 0.1749 - acc: 0.9387 - val_loss: 0.3895 - val_acc: 0.8788
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.0/img/post/acc-loss-model3-cnn.png" alt="acc-loss-model3-cnn.png"></p>
<p>通过加入池化层<code>MaxPooling1D</code>，降低了过拟合的情况。验证集上的准确度超过了前两个模型，也超过了<a href="https://lijqhs.github.io/2019/05/text-classification-scikit-learn/">上篇文章</a>中介绍的传统机器学习方法。</p>
<p>完整代码在<a href="https://github.com/lijqhs/text-classification-cn">GitHub</a>.</p>
<h2 id="6-参考资料">6. 参考资料</h2>
<ul>
<li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep Learning, NLP, and Representations</a></li>
<li><a href="https://keras.io/layers/embeddings/">Keras Embedding Layers API</a></li>
<li><a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/">How to Use Word Embedding Layers for Deep Learning with Keras</a></li>
<li><a href="https://realpython.com/python-keras-text-classification/">Practical Text Classification With Python and Keras</a></li>
<li><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">Francois Chollet: Using pre-trained word embeddings in a Keras model</a></li>
</ul>
              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/nlp/">nlp</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/text-classification/">text classification</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/keras/">keras</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/machine-learning/">machine learning</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/deep-learning/">deep learning</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/embedding/">embedding</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/tf-idf/">tf-idf</a>

  <a class="tag tag--primary tag--small" href="https://lijqhs.github.io/tags/python/">python</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://lijqhs.github.io/2020/10/bias-variance-tradeoff/" data-tooltip="机器学习中的偏差(Bias)与方差(Variance)">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://lijqhs.github.io/2019/05/text-classification-scikit-learn/" data-tooltip="基于搜狗新闻语料的文本分类">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Aaron. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://lijqhs.github.io/2020/10/bias-variance-tradeoff/" data-tooltip="机器学习中的偏差(Bias)与方差(Variance)">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://lijqhs.github.io/2019/05/text-classification-scikit-learn/" data-tooltip="基于搜狗新闻语料的文本分类">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://lijqhs.github.io/2019/05/text-classification-pretrained-keras-cnn/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Flijqhs.github.io%2F2019%2F05%2Ftext-classification-pretrained-keras-cnn%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Flijqhs.github.io%2F2019%2F05%2Ftext-classification-pretrained-keras-cnn%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://cdn.jsdelivr.net/gh/lijqhs/cdn@1.2/img/icons/plane-icon.png" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Aaron</h4>
    
      <div id="about-card-bio">a humble learner</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        data scientist
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        nanjing
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://lijqhs.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://lijqhs.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
  


  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      CommonHTML: { linebreaks: { automatic: true } },
      tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
      messageStyle: 'none'
    });
  </script>



    
  </body>
</html>

